{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U langchain langchain-community faiss-cpu sentence-transformers python-dotenv langchain-google-genai gTTS playsound python-vlc langchain-groq -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:00:57.892356Z","iopub.execute_input":"2025-03-21T12:00:57.892734Z","iopub.status.idle":"2025-03-21T12:01:20.479061Z","shell.execute_reply.started":"2025-03-21T12:00:57.892699Z","shell.execute_reply":"2025-03-21T12:01:20.477107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re \nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.docstore.document import Document\nfrom transformers import pipeline\nimport google.generativeai as genai\nfrom langchain_groq import ChatGroq\nfrom groq import Groq\nimport requests\nfrom bs4 import BeautifulSoup\nimport time\nimport random\nfrom urllib.parse import quote_plus, urlparse\nimport os\nimport sys\nfrom gtts import gTTS\nimport time\nfrom dotenv import load_dotenv\nimport json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:01:20.480849Z","iopub.execute_input":"2025-03-21T12:01:20.481378Z","iopub.status.idle":"2025-03-21T12:01:52.637771Z","shell.execute_reply.started":"2025-03-21T12:01:20.481323Z","shell.execute_reply":"2025-03-21T12:01:52.636583Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"GOOGLE_API_KEY    = \nGROQ_API_KEY      = \nLANGCHAIN_API_KEY = \n\n\n# load_dotenv(\"\")  \n\n# GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n# GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n# LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n\n\nos.environ['GROQ_API_KEY'] = GROQ_API_KEY\nos.environ['LANGCHAIN_API_KEY'] = LANGCHAIN_API_KEY\n\ngenai.configure(api_key=GOOGLE_API_KEY)\nclient = Groq()\nmodel_gemini   = genai.GenerativeModel(\"gemini-1.5-flash\")\n\nembedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\ntranslator = pipeline(\"translation_en_to_hi\", model=\"Helsinki-NLP/opus-mt-en-hi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:14:17.365482Z","iopub.execute_input":"2025-03-21T12:14:17.365870Z","iopub.status.idle":"2025-03-21T12:14:20.474369Z","shell.execute_reply.started":"2025-03-21T12:14:17.365840Z","shell.execute_reply":"2025-03-21T12:14:20.472181Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# WEB SCRAPPER","metadata":{}},{"cell_type":"code","source":"def is_valid_url(url):\n    if not url:\n        return False\n\n    excluded_domains = [\n        'google.com', 'youtube.com', 'facebook.com', 'twitter.com', \n        'instagram.com', 'tiktok.com', 'linkedin.com', 'reddit.com'\n    ]\n\n    try:\n        domain = urlparse(url).netloc.lower()\n        if domain.startswith('www.'):\n            domain = domain[4:]\n        return not any(domain.endswith(excluded) for excluded in excluded_domains)\n    except:\n        return False\n\n\ndef get_random_headers():\n    # random headers to avoid detection.\n    user_agents = [\n        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36',\n        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15',\n        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0'\n    ]\n    return {\n        'User-Agent': random.choice(user_agents),\n        'Accept-Language': 'en-US,en;q=0.9',\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n    }\n\n\ndef is_english_content(content):\n    # for now I will only consider english webpages\n    if not content:\n        return False\n\n    english_words = ['the', 'and', 'for', 'that', 'with', 'this']\n    text_lower = content.lower()\n\n    word_count = sum(1 for word in english_words if f\" {word} \" in text_lower)\n    return word_count >= 3\n\n\ndef direct_news_search(keyword):\n    urls = []\n    encoded_keyword = quote_plus(keyword)\n\n    # directly accessing news sources as google and bing gave errors when number of requests were too many\n    news_sites = [\n        f\"https://www.reuters.com/search/news?blob={encoded_keyword}\",\n        f\"https://www.bloomberg.com/search?query={encoded_keyword}\",\n        f\"https://www.cnbc.com/search/?query={encoded_keyword}\",\n        f\"https://www.bbc.com/news/search?q={encoded_keyword}\",\n        f\"https://www.nytimes.com/search?query={encoded_keyword}\",\n        f\"https://www.theguardian.com/search?q={encoded_keyword}&type=article\"\n    ]\n    urls.extend(news_sites)\n\n    company_domain = keyword.lower().replace(' ', '')\n    company_urls = [\n        f\"https://www.{company_domain}.com\",\n        f\"https://en.wikipedia.org/wiki/{encoded_keyword}\"\n    ]\n    urls.extend(company_urls)\n\n    return urls\n\n\ndef search_duckduckgo(keyword):\n    #using DuckDuckGo for getting keyword and  URLs\n    urls = []\n    search_query = f\"{keyword} news\"\n    try:\n        ddg_url = f\"https://html.duckduckgo.com/html/?q={quote_plus(search_query)}\"\n        headers = get_random_headers()\n        response = requests.get(ddg_url, headers=headers, timeout=10)\n        if response.status_code == 200:\n            soup = BeautifulSoup(response.text, 'html.parser')\n            for result in soup.select('.result__a'):\n                href = result.get('href')\n                if href and '/uddg=' in href:\n                    url = href.split('/uddg=')[1].split('&')[0]\n                    url = requests.utils.unquote(url)\n                    if is_valid_url(url):\n                        urls.append(url)\n    except Exception as e:\n        print(f\"DuckDuckGo search error: {e}\")\n    return urls\n\n\ndef search_bing(keyword):\n    urls = []\n    search_query = f\"{keyword} news\"\n    try:\n        bing_url = f\"https://www.bing.com/search?q={quote_plus(search_query)}&setlang=en\"\n        headers = get_random_headers()\n        response = requests.get(bing_url, headers=headers, timeout=10)\n        if response.status_code == 200:\n            soup = BeautifulSoup(response.text, 'html.parser')\n            for link in soup.select('a[href^=\"http\"]'):\n                url = link.get('href')\n                if url and is_valid_url(url):\n                    urls.append(url)\n    except Exception as e:\n        print(f\"Bing search error: {e}\")\n    return urls\n\n\ndef extract_full_content(url):\n    try:\n        headers = get_random_headers()\n        response = requests.get(url, headers=headers, timeout=15)\n        if response.status_code == 200:\n            soup = BeautifulSoup(response.text, 'html.parser')\n            content = \" \"\n\n            #  title\n            if soup.title:\n                content += f\"TITLE: {soup.title.string.strip()}\\n\\n\"\n\n            #  metadata\n            meta_tags = {}\n            for meta in soup.find_all('meta'):\n                name = meta.get('name', meta.get('property', '')).lower()\n                if name and 'content' in meta.attrs:\n                    meta_tags[name] = meta['content']\n\n            important_meta = ['description', 'keywords', 'author', 'date', \n                              'article:published_time', 'og:title', 'og:description']\n            for meta_name in important_meta:\n                if meta_name in meta_tags:\n                    content += f\"METADATA {meta_name.upper()}: {meta_tags[meta_name]}\\n\"\n            content += \"\\n\"\n\n            #  headings\n            for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n                text = heading.get_text(strip=True)\n                if text:\n                    content += f\"{heading.name.upper()}: {text}\\n\"\n            content += \"\\nMAIN CONTENT:\\n\"\n          \n            text_blocks = set()  # To avoid duplicates\n\n            content_containers = soup.find_all(['article', 'main']) or []\n\n            # all kinds of html elements are checked here \n\n            content_patterns = ['content', 'article', 'story', 'news', 'body', 'text']\n            for pattern in content_patterns:\n                elements = soup.find_all(class_=re.compile(f\".*{pattern}.*\", re.I))\n                elements.extend(soup.find_all(id=re.compile(f\".*{pattern}.*\", re.I)))\n                content_containers.extend(elements)\n\n            if content_containers:\n                for container in content_containers:\n                    for p in container.find_all('p'):\n                        text = p.get_text(strip=True)\n                        if text and len(text) > 30 and text not in text_blocks:\n                            content += f\"{text}\\n\\n\"\n                            text_blocks.add(text)\n                    for div in container.find_all('div'):\n                        if any(c in str(div.get('class', [])) for c in ['nav', 'menu', 'header', 'footer']):\n                            continue\n                        text = div.get_text(strip=True)\n                        if text and len(text) > 100 and text not in text_blocks:\n                            content += f\"{text}\\n\\n\"\n                            text_blocks.add(text)\n\n            if len(text_blocks) < 5:\n                for p in soup.find_all('p'):\n                    text = p.get_text(strip=True)\n                    if text and len(text) > 30 and text not in text_blocks:\n                        content += f\"{text}\\n\\n\"\n                        text_blocks.add(text)\n            return content\n    except Exception as e:\n        print(f\"Error extracting content from {url}: {e}\")\n    return None\n\n\ndef get_articles(company_keyword, num_articles=10):\n\n    # use all above helper functions, uses all functions to handle cases when number of articles obtained is lesser than required\n    \n    list_articles = []\n    urls = []\n\n    # check for URLs using multiple methods\n    urls.extend(search_duckduckgo(company_keyword))\n    # try bing\n    if len(urls) < num_articles * 2:\n        new_urls = search_bing(company_keyword)\n        urls.extend([url for url in new_urls if url not in urls])\n    \n    # if not enough, try news sites directly\n    if len(urls) < num_articles * 2:\n        new_urls = direct_news_search(company_keyword)\n        urls.extend([url for url in new_urls if url not in urls])\n\n    # go to each URL and extract content\n    for url in urls:\n        if len(list_articles) >= num_articles:\n            break\n        try:\n            #  delay to avoid rate limiting\n            time.sleep(1 + random.random())\n            content = extract_full_content(url)\n            if content and is_english_content(content):\n                list_articles.append(content)\n        except Exception as e:\n            print(f\"Error processing {url}: {e}\")\n            continue\n\n    # if still number of articles needd is not achieved, just putting some random placeholders\n    while len(list_articles) < num_articles:\n        list_articles.append(f\"Unable to retrieve content #{len(list_articles)+1} for {company_keyword}.\")\n    \n    return list_articles[:num_articles]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:02:08.611105Z","iopub.execute_input":"2025-03-21T12:02:08.614330Z","iopub.status.idle":"2025-03-21T12:02:08.641685Z","shell.execute_reply.started":"2025-03-21T12:02:08.614280Z","shell.execute_reply":"2025-03-21T12:02:08.640372Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TEXT TO SPEECH ","metadata":{}},{"cell_type":"code","source":"def text_to_speech(text, output_path, language='hi'):\n    try:\n        if os.path.isdir(output_path) or not os.path.splitext(output_path)[1]:\n            output_path = os.path.join(output_path, \"output.mp3\")\n        \n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n        \n        tts = gTTS(text=text, lang=language, slow=False)\n        tts.save(output_path)\n        \n        print(f\"Audio saved to {output_path}\")\n        return output_path\n\n    except Exception as e:\n        print(f\"Error in text to speech conversion: {e}\", file=sys.stderr)\n        return None\n\n\ndef play_mp3(filepath):\n    # the package used varies on the environment used , ipynb vs py file\n    #  if env is jupyter nb\n    try:\n        shell = get_ipython().__class__.__name__\n        if shell == 'ZMQInteractiveShell':  # Jupyter Notebook or qtconsole\n            from IPython.display import Audio, display\n            display(Audio(filepath, autoplay=True))\n            return\n    except NameError:\n        # get_ipython() is not defined, so we are not in a Jupyter Notebook\n        pass\n\n    # normal py file\n    try:\n        from playsound import playsound\n        playsound(filepath)\n    except ImportError:\n        #  playsound installation gave me errors few times while deploying, so I try using python-vlc.\n        try:\n            import vlc\n            player = vlc.MediaPlayer(filepath)\n            player.play()\n            # wait till playback finishes or an error occurs usually\n            while True:\n                state = player.get_state()\n                if state in (vlc.State.Ended, vlc.State.Error):\n                    break\n                time.sleep(0.5)\n        except ImportError:\n            print(\"Please install either 'playsound' or 'python-vlc' to play audio.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:02:08.643651Z","iopub.execute_input":"2025-03-21T12:02:08.643958Z","iopub.status.idle":"2025-03-21T12:02:08.671329Z","shell.execute_reply.started":"2025-03-21T12:02:08.643933Z","shell.execute_reply":"2025-03-21T12:02:08.669841Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CHAT FUNCTIONS (To process the article)","metadata":{}},{"cell_type":"code","source":"def chat(question , model=\"gemini\"):\n\n    if model==\"llama\":\n        chat_completion = client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": question\n            }\n        ],\n        model=\"llama-3.3-70b-versatile\",\n        )\n        answer = chat_completion.choices[0].message.content\n        return answer\n\n    elif model==\"gemini\":\n        answer = model_gemini.generate_content(\n            question, \n            generation_config = genai.GenerationConfig(\n            max_output_tokens=8192,\n            temperature=0.8,\n            )\n        )\n        return answer.text\n\ndef hindi(text: str) -> str:\n    try:\n        translation = translator(text, max_length=20480)\n        hindi_text = translation[0]['translation_text']\n        return hindi_text\n    except Exception as e:\n        print(f\"Translation error: {e}\")\n        return \"\"\n\ndef review_company(all_articles):\n    instruction = f\"\"\"\n    \n    Given below are a number of articles about a company and a positive vs negative feedback assigned to each article of the company found on internet.\n    The number of positive and negative articles I will give you may or may not be equal.\n    I want you to compare all the articles and write a detailed final review about the company, stating your reasons for if it is really good (positive) or not good (negative).\n    If you don't find a strong review based on the content I have given, write a plausible and relevant review for the company\n    \n    After that enclose the final review within the following: \n    The example text I gave should be replaced with the actual review \n    boxed_final_review{{\"Tesla’s latest news coverage is mostly positive beacuse\"}}\n    \n    I will be using regex pattern so you should ensure your response follows the format I asked     \n\n    Here are all the articles I have for you to go through:\n    {all_articles}\n    \n    Follow all instructions carefully and produce the final review I have asked. This will be the end of your response. \n    \"\"\"\n    \n    response = chat(instruction)\n    \n    print(\"response\" , response[:100])\n\n    match = re.search(r'boxed_final_review\\s*{\\s*\"(.*?)\"\\s*}', response, re.DOTALL)\n\n    if match:\n        final_review = match.group(1).strip()  # Extract review text\n    else:\n        final_review = \"No concrete review found\"\n\n    print(\"Extracted Review:\", final_review[:40])\n    \n\n    return final_review\n\n\ndef coverage_diff(all_articles):\n    \n    instruction = f\"\"\"\n    \n    Given below are a number of articles about a company found on the internet. For each article I want you to:\n    1) Determine it's impact in terms of portraying details about the company\n    2) Go each article and compare with any other article on which we can draw some comparisons\n\n    So for each article get the impact of the article and comparison with other articles \n    You can atmost compare it with 2 articles so make sure other articles picked are well suited for comparison with the article being compared.\n    So all the impacts and comparions should be added to 2 separate lists like this :\n    For example: \n    \n    IMPACT_LIST = [\"Article 1 highlights Tesla's strong sales, while Article 2 discusses regulatory issues.\",  \"Article 3 is focused on financial success and innovation,whereas Article 4 is about legal challenges and risks.\"]\n    COMPARISON_LIST = [\"The first article boosts confidence in Tesla's market growth, while the second raises concerns about future regulatory hurdles.\" , \"Investors may react positively to growth news but stay cautious due to regulatory scrutiny.\"]\n    \n    So make sure this way of assigning is done and don't change the names of lists I gave because i will be using these regex patterns to extract these 2 lists\n\n    Make sure the length of the 2 lists are same and also equal to the number of articles I have given. If in any extreme case you fail to assign the impact and comparison of any article, use any relevant knowledge you know to assign something for the impact and comparison, don't leave it empty.    \n\n    These are the articles you need to go through:\n    {all_articles}\n    \n    Follow all instructions carefully and produce the 2 lists I have asked. This will be the end of your response. \n\n    \"\"\"\n\n    response = chat(instruction)\n\n    impact_list_pattern     = r'IMPACT_LIST\\s*=\\s*\\[(.*?)\\]'\n    comparison_list_pattern = r'COMPARISON_LIST\\s*=\\s*\\[(.*?)\\]'\n\n\n    impact_list_match = re.search(impact_list_pattern, response, re.DOTALL)\n    impact_list       = re.findall(r'\"(.*?)\"', impact_list_match.group(1)) if impact_list_match else [\"No impacts found\"]\n    \n    comparison_list_match = re.search(comparison_list_pattern, response, re.DOTALL)\n    comparison_list       = re.findall(r'\"(.*?)\"', comparison_list_match.group(1)) if comparison_list_match else [\"No comparisons found\"]\n\n    return impact_list , comparison_list\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:09:23.718913Z","iopub.execute_input":"2025-03-21T12:09:23.719335Z","iopub.status.idle":"2025-03-21T12:09:23.731697Z","shell.execute_reply.started":"2025-03-21T12:09:23.719307Z","shell.execute_reply":"2025-03-21T12:09:23.730047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_article(article):\n\n    instruction = f\"\"\"\n    \n    Given below is an article content about a company. Read the content thoroughly and perform the following:\n    1) The title of the article should be extratced if present. If it is not present give me a suitable title for the article given.\n    2) A very informative summary of the article should be made with good detials and length, do not miss any important point present while writing the summary.\n    3) A sentiment which : either \"Positive\" or \"Negative\" should be assigned to the article I have sent you.\n    4) The list of topics that the article covers should be put in a list.\n    \n    So after processing and producing these 4. You put them in the following format, check the example I have given below. This is very important \n    boxed_title{{\"Tesla's New Model Breaks Sales Records\"}}\n    boxed_summary{{\"Tesla's latest EV sees record sales in the year...\"}}\n    boxed_sentiment{{\"Negative\"}}\n    LIST_TOPICS = [\"Electric Vehicles\", \"Stock Market\", \"Innovation\"]\n\n    The topics you extracted should be placed in a list and assigned = to the LIST_TOPICS variable, do not change name here. \n    This is very important as I will be using these particular regex pattern to extract the 4 things I listed, so make sure your response will work with the below regex patterns. \n    \n\n    The LIST_TOPICS equal to should be present like I asked.\n\n    Below is the article on which you to perform the tasks I have asked you\n    \n    Article:\n    {article}\n\n    Follow all instructions carefully and produce the 4 components I have asked. This will be the end of your response. \n    \"\"\"\n    response = chat(instruction)\n    \n\n    boxed_title     = re.search(r'boxed_title\\s*{\\s*\"(.+?)\"\\s*}', response)\n    boxed_summary   = re.search(r'boxed_summary\\s*{\\s*\"(.+?)\"\\s*}', response)\n    boxed_sentiment = re.search(r'boxed_sentiment\\s*{\\s*\"(.+?)\"\\s*}', response)\n\n    list_topics_match = re.search(r'LIST_TOPICS\\s*=\\s*\\[(.*?)\\]', response, re.DOTALL)\n    list_topics       = re.findall(r'\"(.*?)\"', list_topics_match.group(1)) if list_topics_match else [\"No topics found\"]\n\n    title      = boxed_title.group(1) if boxed_title else \"No title\"\n    summary    = boxed_summary.group(1) if boxed_summary else \"No summary\"\n    sentiment  = boxed_sentiment.group(1) if boxed_sentiment else \"No sentiment\"\n\n    return title, summary, sentiment, list_topics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:09:23.925380Z","iopub.execute_input":"2025-03-21T12:09:23.925721Z","iopub.status.idle":"2025-03-21T12:09:23.933373Z","shell.execute_reply.started":"2025-03-21T12:09:23.925693Z","shell.execute_reply":"2025-03-21T12:09:23.931682Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GETTING REPORT OF  ARTICLE (Using all functions above)","metadata":{}},{"cell_type":"code","source":"def get_output(company):\n\n    all_articles_list = []\n    sentiment_distibution = {}\n    articles_with_score = \" \"\n    articles_without_score = \" \"\n    list_of_topics = []\n    \n    articles  = get_articles(company , num_articles=1)\n    print(\"ALL ARTICLES GATHERED\")\n    \n    for i,article in enumerate(articles):\n        title , summary , sentiment , list_topics = process_article(article)\n        article_dict = {\n            \"Title\" : title,\n            \"Summary\" : summary , \n            \"Sentiment\" : sentiment , \n            \"Topics\" : list_topics\n        }\n        all_articles_list.append(article_dict)\n        sentiment_distibution[sentiment] = sentiment_distibution.get(sentiment,0)+1\n        list_of_topics.append(list_topics)\n\n        articles_with_score+=  f\"Article review : {sentiment} Article content: {article}\"+ \"\\n\"\n        articles_without_score+= f\"Article number: {i} Article content:{article}\" + \"\\n\"\n    print(\"ARTICLES summary, title, sentiment processed\")\n\n    final_review  =  review_company(articles_with_score)\n    impact_list , comparison_list  = coverage_diff(articles_without_score)\n    print(\"ARTICLES review and impact processed\")\n    \n    common_topics = set(list_of_topics[0])\n    for article in list_of_topics[1:]:\n        common_topics.intersection_update(article)\n    \n    if not common_topics:\n        common_list_of_topics = [\"no common topics\"]\n    else:\n        common_list_of_topics = sorted(common_topics)\n    \n    topic_overlap = {\"Common Topics\": common_list_of_topics}\n    for index, article in enumerate(list_of_topics, start=1):\n        unique_topics = sorted(set(article) - common_topics)\n        topic_overlap[f\"Unique Topics in Article {index}\"] = unique_topics\n\n    \n    coverage_dicts_list = []\n    for i , (impact, comparison) in enumerate(zip(impact_list , comparison_list)):\n        dic = {\n            f\"Comparison Article {i} \" : comparison , \n            f\"Impact of Article {i} \" : impact\n        }\n        coverage_dicts_list.append(dic)\n\n    hindi_review = hindi(final_review)\n    print(\"ARTICLES review processed\")\n    \n    output = {\n\n        \"Company\"  :company,                 \n        \"Articles\" :all_articles_list,      \n        \"Comparative Sentiment Score\": { \n            \"Sentiment Distribution\" : sentiment_distibution,\n            \"Coverage differences\"   :  coverage_dicts_list, \n            \"Topic overlap\"          :  topic_overlap\n        },\n        \"Final sentiment analysis\" : final_review,\n        \"Hindi summary\" : hindi_review,\n        \"Article corpus\" : articles_without_score , \n    }\n    \n    return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:09:24.258197Z","iopub.execute_input":"2025-03-21T12:09:24.258535Z","iopub.status.idle":"2025-03-21T12:09:24.268790Z","shell.execute_reply.started":"2025-03-21T12:09:24.258511Z","shell.execute_reply":"2025-03-21T12:09:24.267278Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RAG SYSTEM","metadata":{}},{"cell_type":"code","source":"def save_vector_store(corpus:str , chunk_size: int = 500, chunk_overlap: int = 50, save_path=\"/kaggle/working/\"):\n    \n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n    chunks = text_splitter.split_text(corpus)\n    \n    documents = [Document(page_content=chunk) for chunk in chunks]\n    \n    vector_store = FAISS.from_documents(documents, embedding_model)\n    vector_store.save_local(save_path)\n\n    print(\"Saved vectorstore\")\n    return None\n\ndef retrieve_relevant_text( question: str,  top_k: int = 5 , save_path=\"/kaggle/working/\") -> str:\n\n    vector_store = FAISS.load_local(save_path, embedding_model, allow_dangerous_deserialization=True)\n\n    retrieved_docs = vector_store.similarity_search(question, k=top_k)\n    \n    retrieved_text = \" \".join([doc.page_content for doc in retrieved_docs])\n    \n    return retrieved_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:09:24.561865Z","iopub.execute_input":"2025-03-21T12:09:24.562309Z","iopub.status.idle":"2025-03-21T12:09:24.569653Z","shell.execute_reply.started":"2025-03-21T12:09:24.562276Z","shell.execute_reply":"2025-03-21T12:09:24.568214Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# QUERY/INDEXING","metadata":{}},{"cell_type":"code","source":"\ndef chat_questions(user_question , corpus , model=\"gemini\" , mode=\"simple\" ):\n\n    if mode==\"simple\":\n        content = corpus\n\n    elif mode==\"advanced\":\n        content =  retrieve_relevant_text(question)\n\n        \n    question = f\"\"\"\n    Question:\n    {user_question}\n\n    Use the following content below to search your answer. If no answer is found from the content, give the most suitable answer from your knowledge\n    Also give a small description of the company along with it. \n    Content:\n    {content}\n    \n    \"\"\"\n    answer =  chat(question , model)\n\n    return answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:13:46.047976Z","iopub.execute_input":"2025-03-21T12:13:46.048403Z","iopub.status.idle":"2025-03-21T12:13:46.053963Z","shell.execute_reply.started":"2025-03-21T12:13:46.048368Z","shell.execute_reply":"2025-03-21T12:13:46.052537Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"company =  \"Gooogle\"\noutput = get_output(company)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:12:21.612642Z","iopub.execute_input":"2025-03-21T12:12:21.613011Z","iopub.status.idle":"2025-03-21T12:12:37.596904Z","shell.execute_reply.started":"2025-03-21T12:12:21.612979Z","shell.execute_reply":"2025-03-21T12:12:37.595528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file_path = \"output.json\"\nwith open(file_path, \"w\") as json_file:\n    json.dump(output, json_file, indent=4)\n\ncorpus     =  output[\"Article corpus\"]\nhindi_text =  output[\"Hindi summary\"]\n\nsave_vector_store(corpus)\n\noutput_file_path = \"/kaggle/working\" \nresult = text_to_speech(hindi_text, output_file_path)\n\nplay_mp3(\"/kaggle/working/output.mp3\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:15:56.031833Z","iopub.execute_input":"2025-03-21T12:15:56.032215Z","iopub.status.idle":"2025-03-21T12:15:56.037559Z","shell.execute_reply.started":"2025-03-21T12:15:56.032185Z","shell.execute_reply":"2025-03-21T12:15:56.035869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mode=\"simple\"\nif mode==\"simple\":\n    answer = chat_questions(\"Tell me something about google\" ,corpus ,  model=\"gemini\" )\nelif mode==\"advanced\":\n    answer = chat_questions(\"Tell me something about google\" )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:17:49.314034Z","iopub.execute_input":"2025-03-21T12:17:49.314471Z","iopub.status.idle":"2025-03-21T12:17:50.525155Z","shell.execute_reply.started":"2025-03-21T12:17:49.314440Z","shell.execute_reply":"2025-03-21T12:17:50.523784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:18:03.362909Z","iopub.execute_input":"2025-03-21T12:18:03.363365Z","iopub.status.idle":"2025-03-21T12:18:03.370340Z","shell.execute_reply.started":"2025-03-21T12:18:03.363333Z","shell.execute_reply":"2025-03-21T12:18:03.369110Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}